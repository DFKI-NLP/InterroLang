model_name,model_summary,task,epochs,lr,loss,optimizer,train_data_name,train_data_source,train_data_language,train_data_number,test_data_name,test_data_source,test_data_language,test_data_number
BertForSequenceClassification,Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.,Hate Speech Detection,3,0.00005,BCEWithLogitsLoss,Adam,OLID Dataset: Offensive Language Identification Dataset,tweet,English,2000,OLID Dataset: Offensive Language Identification Dataset,tweet,English,1000
Distilbert-base-uncased-qa-boolq,This model is a distilled version of the BERT base model. It was introduced in this paper. The code for the distillation process can be found here. This model is uncased: it does not make a difference between english and English.,Question Answering,5,0.00005,,Adam,BoolQ Dataset: a question answering dataset for yes/no questions,plain text,English,9427,BoolQ Dataset: a question answering dataset for yes/no questions,plain text,English,3270
BertForSequenceClassification,Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.,Dialog Act Classification,5,0.00005,CrossEntropyLoss,AdamW,DailyDialog Dataset: multi-turn dialog ,dialog,English,11118,DailyDialog Dataset: multi-turn dialog ,dialog,English,1000
